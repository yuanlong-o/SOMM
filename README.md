# A Systematically Optimized Miniaturized Mesoscope (SOMM) for large-scale calcium imaging in freely moving mice
<img src="img/deepwonder_logo.png" width="800" align="center">
Implementation for deep widefield neuron finder (DeepWonder)

![Imaging modality](https://img.shields.io/badge/Imaging%20modality-Wide--field%20Single--photon-brightgreen)  ![Purpose](https://img.shields.io/badge/Purpose-Neuron%20analysis-orange)  

## üìã Table of content
 1. [Overview](#Overview)
 2. [DOE optimization](#DOE)
    1. [Prepare environment](#Environment)
    1. [Generate Zernike polynomals](#zernike)
    2. [Generate NAOMi samples](#naomi)
    3. [Run the optimization](#run)
 3. [Mechanical part details](#Train)
    1. [Realistic widefield capture generation](#NAOMI)
    2. [Background removal network training](#TrainBR)
    3. [Neuron segmentation network training](#TrainNS)
 4. [Processing code](#Information)
    1. [Citation](#Citation)
    2. [Email](#Email)

## **üìö** Overview <a name="Overview"></a>
Interrogating neural circuits in freely behaving mammals is poised to shed a light on the neuronal systems dynamics underlying complex naturalistic behaviors. However, optical recording of neuronal activity in freely behaving animals has remained limited to a small scale and is vulnerable to motion-induced focus drifting. Here, we present a systematically optimized miniaturized mesoscope (SOMM), a widefield, head-mounted fluorescent mesoscope that overcomes these obstacles and allows imaging during free behavior at mesoscopic field-of-view, single-cell resolution, with uniform illumination, and robust axial accessibility. Powered by compact diffractive optics and associated computational algorithms, SOMM can capture neuronal network activity within a field-of-view of 3.6 √ó 3.6 mm¬¨2 at 4 ¬µm resolution and at up to 16 Hz in the cortex of freely moving mice, with great defocus tolerance across 300 ¬µm and a weight of less than 2.5 g. Using SOMM, we recorded large-scale population activity during social interactions, cross-region neuronal activity evoked by visual and electrical stimuli, and neurovascular coupling in dual-color, all at single-cell spatial resolution and physiologically relevant temporal resolution 


## **‚è≥** DOE optimization <a name="DOE"></a>
In this part we introduce the DOE optimization in SOMM. 
### **üí°** Environment <a name="Environment"></a>
* Ubuntu 16.04 
* Python 3.6
* tnesorflow = 1.14
* NVIDIA GPU (24 GB Memory) + CUDA

### **üí°** Generate zernike polynomials <a name="zernike"></a>
* run DOE_optimization\gen_zernike_polynomial.m to generate Zernike basis for optimization

### **üí°** Generate NAOMi samples for training <a name="naomi"></a>
* Code for generating NAOMi samples built for one-photon (or widefield) imaging modality can be found in https://github.com/yuanlong-o/Deep_widefield_cal_inferece

### **üí°** Run optimization for DOE <a name="run"></a>
* Run main_LFOV_DOE_train.py to train a DOE and corresponding decovnolution algorithm for large FOV capability and depth robustness. Optical parameters should be corresondingly modified for different systems.
* The output phase would 


## **üîÅ** Train DeepWonder <a name="Train"></a>

### **üí°** Realistic widefield capture generation <a name="NAOMI"></a>
DeepWonder relies on a highly realistic simulation of widefield capture for training a network that removes widefield background. We referred to the Neural Anatomy and Optical Microscopy (NAOMi) package (https://www.sciencedirect.com/science/article/pii/S0165027021001084) to populate the brain tissue with multiple blood vessels, somata, axons, and dendrites. We further modified the NAOMi pipeline such that it could faithfully simulate data acquisition of one-photon excitations with one-photon excitation model and related noise model, which we termed as NAOMi1p. The virtual widefield data by NAOMi1p can be affected by optical parameters, illumination powers, and other parameters like protein concentration. The full key parameters are listed as
1. Optical parameters of the microscope: NA, FOV, FN, immersion medium
2. Indicator parameters: expression level, indicator types (e.g. GCaMP6 or GCaMP7)
3. Imaging parameters: session length, frame rate, illumination powers, and imaging depth
4. (optional) Vessel dilation parameters: dilation dynamics, amplitude, and anisotropy across the FOV.

All those parameters should be adjusted based on a specific system. As an example, run NAOMi_1p_single.m to generate a 750 x 750 x 1000 frame virtual widefield recording (mov_w_bg.tiff) along with a background free pair (mov_wo_bg.tiff). To generate multiple training pairs, run NAOMi_1p_loop.m which nests a for loop to generate data.


### **üí°** Train background removal network <a name="TrainBR"></a>
Put the backaground removed movie generated by NAOMI1p to the *DeepWonder/datasets/XXXX/GT* folder, and the paired background contaminated movie to the *DeepWonder/datasets/XXXX/Input* folder. After preparing the data, run the script_train_RMBG.py to train the background removal network.
```
$ source activate deepwonder_env
$ python script_train_RMBG.py train
```
The trained removing background model will show up in *DeepWonder/RMBG_pth* folder.
We also provide pre-trained models to speed up the fine tuning process. This model is fully trained on a large number of simulated data with different characteristics and can be used as the initialization of the background removal network model. Run the script_train_RMBG.py to train the background removal network with pretrain model initilaization.
```
$ python script_train_RMBG_conti.py train
```

### **üí°** Train neuron segmentation network <a name="TrainNS"></a>
Put the backagrond removed movie generated to the *DeepWonder/datasets/XXXX/image* folder as inputs, and the corresponding segmented masks to the *DeepWonder/datasets/XXXX/mask* folder as labels. A good option to get the mask data is to binarize simulated neurons for each of frames (individual neurons are in the vol_out structure from NAOMi1p output). Alternatively, running other two-photon segmentation tools (like CaImAn https://github.com/flatironinstitute/CaImAn) to get segments can also work but probably with performance dropping. Run the script_train_SEG.py to train the neuron segmentation network.
```
$ source activate deepwonder_env
$ python script_train_SEG.py train
```
The trained neuron segmentation model will show up in *DeepWonder/SEG_pth* folder.
 
## ü§ù Other information <a name="Information"></a>
### **üìù** Citation <a name="Citation"></a>

   
### **üìù** Email <a name="Email"></a>
We are pleased to address any questions regarding the above tools through emails (zhanggx19@mails.tsinghua.edu.cn or ylzhang16@mails.tsinghua.edu.cn).
